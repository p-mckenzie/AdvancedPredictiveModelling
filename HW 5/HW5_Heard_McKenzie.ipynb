{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">MIS382: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 5</p>\n",
    "## <p style=\"text-align: center;\">Paige McKenzie (pam2932), Yannick Heard (yoh64)</p>\n",
    "## <p style=\"text-align: center;\">Total points: 35</p>\n",
    "## <p style=\"text-align: center;\">Due: Monday, November 27th, submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Ensembles (1+12+2 = 15pts)\n",
    "In this question, we will compare performance of different ensemble methods: [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [XGBoost](http://xgboost.readthedocs.io/en/latest/).  Note that you have to install xgboost package in addition to scikit-learn.  You can see installation guides [here](http://xgboost.readthedocs.io/en/latest/build.html).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Two  datasets are provided for this problem. For **each of the datasets ((X1.csv, y1.csv), (X2.csv, y2.csv))**, do the following:\n",
    "\n",
    "1. Load the data and partition it into features (X) and the target label (y) for classification task. Then, use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split data into training and testing: test_size=0.33, random_state=42.\n",
    "\n",
    "2. Build a classifier using [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), and [XGBoost](http://xgboost.readthedocs.io/en/latest/), respectively, and answer the following for each classifier.\n",
    "\n",
    " - Mention any design choices (with reasoning/justification) that you made, e.g. the hyperparameters considered for each classifier.\n",
    " - Report the mean error rate (fraction of incorrect labels) and the confusion matrix on test data. <br>\n",
    " - Report the feature importance and time of execution (training and predicting times).\n",
    "\n",
    "3. Compare the three classifiers for the two different datasets ((X1.csv, y1.csv), (X2.csv, y2.csv)) in terms of the misclassification rate.  What are the characteristics of the dataset and the classifiers that resulted in somewhat different comparative results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\yanni\\\\OneDrive\\\\Documents\\\\GitHub\\\\AdvancedPredictiveModelling\\\\HW 5\\\\hmk5_data\\\\')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = pd.read_csv('X1.csv')\n",
    "y1 = pd.read_csv('y1.csv')\n",
    "X2 = pd.read_csv('X2.csv')\n",
    "y2 = pd.read_csv('y2.csv')\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.33, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanni\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "rfc1 = RandomForestClassifier(15)\n",
    "rfc1.fit(X1_train, y1_train)\n",
    "pred1 = rfc1.predict(X1_test)\n",
    "rfc1acc = metrics.accuracy_score(y1_test,pred1)\n",
    "rfc1cf = confusion_matrix(y1_test, pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbc1 = GradientBoostingClassifier(learning_rate = 0.15, n_estimators = 250, max_depth = 5)\n",
    "gbc1.fit(X1_train, y1_train)\n",
    "pred_gbc1 = gbc1.predict(X1_test)\n",
    "gbc1acc = metrics.accuracy_score(y1_test,pred_gbc1)\n",
    "gbc1cf = confusion_matrix(y1_test, pred_gbc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc1 = XGBClassifier(max_depth = 8)\n",
    "xgbc1.fit(X1_train, y1_train)\n",
    "pred_xgbc1 = xgbc1.predict(X1_test)\n",
    "xgbc1acc = metrics.accuracy_score(y1_test,pred_xgbc1)\n",
    "xgbc1cf = confusion_matrix(y1_test, pred_xgbc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean error rate for random forest classifier is = 0.13696969697 and the confusion matrix is,\n",
      "[[1420  280]\n",
      " [ 172 1428]]\n",
      "The mean error rate for gradient boosting decision tree is = 0.0881818181818 and the confusion matrix is,\n",
      "[[1531  169]\n",
      " [ 122 1478]]\n",
      "The mean error rate for XGBoost Classifier is = 0.0818181818182 and the confusion matrix is,\n",
      "[[1538  162]\n",
      " [ 108 1492]]\n"
     ]
    }
   ],
   "source": [
    "print \"The mean error rate for random forest classifier is =\", 1-rfc1acc, \"and the confusion matrix is,\"\n",
    "print rfc1cf\n",
    "print \"The mean error rate for gradient boosting decision tree is =\", 1-gbc1acc, \"and the confusion matrix is,\"\n",
    "print gbc1cf\n",
    "print \"The mean error rate for XGBoost Classifier is =\", 1-xgbc1acc, \"and the confusion matrix is,\"\n",
    "print xgbc1cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best mean error that was able to be obtained was with the XGBoost Classifier, but only by a small margin over the gradient boosting tree and by more over the random forest classifier. The parameters selected within each of the models used on this dataset were based on multiple attempts and seeing which alterations improved the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanni\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "rfc2 = RandomForestClassifier(15)\n",
    "rfc2.fit(X2_train, y2_train)\n",
    "pred2 = rfc2.predict(X2_test)\n",
    "rfc2acc = metrics.accuracy_score(y2_test,pred2)\n",
    "rfc2cf = confusion_matrix(y2_test, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbc2 = GradientBoostingClassifier(learning_rate = 0.15, n_estimators = 150, max_depth = 5)\n",
    "gbc2.fit(X2_train, y2_train)\n",
    "pred_gbc2 = gbc2.predict(X2_test)\n",
    "gbc2acc = metrics.accuracy_score(y2_test,pred_gbc2)\n",
    "gbc2cf = confusion_matrix(y2_test, pred_gbc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92666666666666664"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc2 = XGBClassifier(max_depth = 8)\n",
    "xgbc2.fit(X2_train, y2_train)\n",
    "pred_xgbc2 = xgbc2.predict(X2_test)\n",
    "xgbc2acc = metrics.accuracy_score(y2_test,pred_xgbc2)\n",
    "xgbc2cf = confusion_matrix(y2_test, pred_xgbc2)\n",
    "xgbc2acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean error rate for random forest classifier is = 0.0939393939394 and the confusion matrix is,\n",
      "[[762  92]\n",
      " [ 63 733]]\n",
      "The mean error rate for gradient boosting decision tree is = 0.0715151515152 and the confusion matrix is,\n",
      "[[775  79]\n",
      " [ 39 757]]\n",
      "The mean error rate for XGBoost Classifier is = 0.0733333333333 and the confusion matrix is,\n",
      "[[770  84]\n",
      " [ 37 759]]\n"
     ]
    }
   ],
   "source": [
    "print \"The mean error rate for random forest classifier is =\", 1-rfc2acc, \"and the confusion matrix is,\"\n",
    "print rfc2cf\n",
    "print \"The mean error rate for gradient boosting decision tree is =\", 1-gbc2acc, \"and the confusion matrix is,\"\n",
    "print gbc2cf\n",
    "print \"The mean error rate for XGBoost Classifier is =\", 1-xgbc2acc, \"and the confusion matrix is,\"\n",
    "print xgbc2cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters selected within each of the models used on this dataset were based on multiple attempts and seeing which alterations improved the test accuracy.\n",
    "The best mean error that was able to be obtained was with the XGBoost Classifier, but only by a small margin over the gradient boosting tree and by more over the random forest classifier. For all of the models used they were all able to obtain better mean error rates on the second dataset than on the first dataset. This difference in accuracy for dataset can be occuring since the second dataset has less variables than the first data set(20 vs30), and the additional variables must add more noise and confusion to the model.\n",
    "The differences in the models from a random forest to the other two is that a random forest involves building full trees and averaging that, while the others involve only building small trees and the combinations of those. The difference between the gradient boosting decision tree and the XGBoost is that XGBoost has a regularized model formation to prevent overfitting on training data allowing for better out of sample results. Other than that the two models are very similar as they use ensembles of weak decision trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Visualization using Bokeh (10 pts)\n",
    "\n",
    "In this problem, you'll build an interactive visualization. Bokeh is a Python interactive visualization library that targets modern web browsers for presentation. For more information on Bokeh, see http://bokeh.pydata.org/en/latest/. The problem statement is as follows:\n",
    "\n",
    "Using the \"nbasalariesfull.csv\" data set from HMK4, your goal is to build a Bokeh visualization which allows the user to explore how salary (on a log scale) varies with points per game (PSG) and age. You will create a visualization that allows the user to toggle the X axis of a scatter plot between PSG and age, with the y-axis always being log Salary. Also add the hover tool so that if the user hovers over a datapoint in the plot a window pops up that shows the player name, team, position, salary, and the current x variable (PSG or age) depending on the current tab.  Color each point according to a player's position and provide a legend for the colors. Add the ability to Zoom in/out.  Add slight horizontal jitter to a player's age.\n",
    "\n",
    "Hints: \n",
    "1. see: http://bokeh.pydata.org/en/latest/docs/user_guide/tools.html#basic-tooltips for hover and zoom tool examples.\n",
    "2. See: http://bokeh.pydata.org/en/latest/docs/reference/plotting.html. Look for the scatter API.\n",
    "3. See: http://bokeh.pydata.org/en/0.10.0/docs/user_guide/styling.html#labels. For labeling axes.\n",
    "4. See: https://bokeh.pydata.org/en/latest/docs/user_guide/categorical.html  for how to use jitter transform\n",
    "5. See: http://bokeh.pydata.org/en/latest/docs/gallery/iris.html for coloring points by category\n",
    "6. Use output_notebook() from Bokeh to output the plot to your notebook\n",
    "\n",
    "Include an image screenshot in addition to the visualization output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bokeh.models.widgets import Panel, Tabs\n",
    "from bokeh.models import CustomJS, ColumnDataSource, HoverTool, BoxZoomTool\n",
    "from bokeh.transform import jitter\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "\n",
    "data = pd.read_csv(\"nbasalariesfull.csv\")\n",
    "data[\"logsalary\"] = data.SALARY.apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Diabetes classification using support vector machines (4+3+3=10 pts) \n",
    "(a) Apply a linear SVM, using the scikit-SVM, for the Pima Indian Women diabetes detection problem on the dataset provided (details on dataset here  http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) . Specify how you chose the slack cost/penalty (‘C’ parameter)for the model. Maintain all other parameters as default. Hint: http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html will make 10-fold cross-validation easier.\n",
    "The code to get the training/testing data is provided below.\n",
    "\n",
    "(b) Repeat (a) but using a Gaussian radial basis kernel.\n",
    "\n",
    "(c) Summarize the comparative performance (mean error rates) of the classifiers. What do you conclude? (be brief)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import grid_search\n",
    "\n",
    "data_train = pd.read_csv('diabetes_train-log.csv')\n",
    "data_test = pd.read_csv('diabetes_test-log.csv')\n",
    "cols = ['numpreg', 'plasmacon', 'bloodpress', 'skinfold', 'seruminsulin', 'BMI', 'pedigreefunction', 'age']\n",
    "\n",
    "xtrain = np.asmatrix(data_train[cols])\n",
    "ytrain = np.asarray(data_train['classvariable']).T\n",
    "\n",
    "xtest = np.asmatrix(data_test[cols])\n",
    "ytest = np.asarray(data_test['classvariable']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
